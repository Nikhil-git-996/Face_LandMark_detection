<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Face and Iris Landmarks</title>
    <style>
        body {
            display: flex;
            justify-content: center;
            align-items: center;
            height: 100vh;
            margin: 0;
            background-color: #f0f0f0;
        }

        #container {
            position: relative;
            display: flex;
            justify-content: center;
            align-items: center;
            width: 640px;
            height: 480px;
        }

        video {
            position: absolute;
            top: 0;
            left: 0;
            width: 640px;
            height: 480px;
            z-index: 1;
        }

        canvas {
            position: absolute;
            top: 0;
            left: 0;
            width: 640px;
            height: 480px;
            z-index: 2;
        }
    </style>
</head>
<body>
<div id="container">
    <video id="video" autoplay muted></video>
    <canvas id="output"></canvas>
</div>

<!-- Load MediaPipe FaceMesh -->
<script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.min.js"></script>

<!-- JavaScript -->
<script>
    const VIDEO_WIDTH = 640;
    const VIDEO_HEIGHT = 480;

    let video, canvas, ctx, faceMesh;
    let leftEyeClosedTime = null;
    let rightEyeClosedTime = null;
    let totalEyeClosureTime = 0; // Total time both eyes are closed
    let alertShown = false;

    // Initialize the camera feed
    async function setupCamera() {
        video = document.getElementById('video');
        const stream = await navigator.mediaDevices.getUserMedia({
            video: { width: VIDEO_WIDTH, height: VIDEO_HEIGHT }
        });
        video.srcObject = stream;
        return new Promise((resolve) => {
            video.onloadedmetadata = () => {
                video.play();
                resolve(video);
            };
        });
    }

    // Initialize MediaPipe FaceMesh
    function initializeFaceMesh() {
        faceMesh = new FaceMesh({
            locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`
        });

        faceMesh.setOptions({
            maxNumFaces: 2, // Allow up to 2 faces for detection
            refineLandmarks: true,
            minDetectionConfidence: 0.5,
            minTrackingConfidence: 0.5
        });

        faceMesh.onResults(onResults);
    }

    // Process results and analyze
    function onResults(results) {
        ctx.clearRect(0, 0, VIDEO_WIDTH, VIDEO_HEIGHT);

        const faces = results.multiFaceLandmarks;

        // Alert for multiple faces
        if (faces && faces.length > 1) {
            displayWarning("Multiple faces detected. Only one person should be in front of the screen.");
            return;
        }

        if (faces) {
            for (const landmarks of faces) {
                drawLandmarks(landmarks, 'red');
                monitorFaceDirection(landmarks);
                monitorEyeClosure(landmarks);
                processEyes(landmarks); // Draw iris
            }
        }

        // Display the eye closure time
        displayEyeClosureTime();
    }

    // Draw landmarks on the canvas
    function drawLandmarks(landmarks, color) {
        ctx.fillStyle = color;
        landmarks.forEach((landmark) => {
            const x = landmark.x * VIDEO_WIDTH; // Scale x-coordinate
            const y = landmark.y * VIDEO_HEIGHT; // Scale y-coordinate
            ctx.beginPath();
            ctx.arc(x, y, 2, 0, 2 * Math.PI);
            ctx.fill();
        });
    }

    // Monitor face direction
    function monitorFaceDirection(landmarks) {
        const noseTip = landmarks[1];
        const noseBase = landmarks[168];
        const faceCenterX = noseBase.x * VIDEO_WIDTH;

        if (faceCenterX < VIDEO_WIDTH * 0.3 || faceCenterX > VIDEO_WIDTH * 0.7) {
            displayWarning("Face not centered. Please look at the screen.");
        }
    }

    // Monitor eye closure
    function monitorEyeClosure(landmarks) {
        const leftEyeLandmarks = [159, 145];
        const rightEyeLandmarks = [386, 374];

        const leftEyeOpen = Math.abs(landmarks[leftEyeLandmarks[0]].y - landmarks[leftEyeLandmarks[1]].y) > 0.01;
        const rightEyeOpen = Math.abs(landmarks[rightEyeLandmarks[0]].y - landmarks[rightEyeLandmarks[1]].y) > 0.01;

        const currentTime = performance.now();

        if (!leftEyeOpen && !rightEyeOpen) {
            if (leftEyeClosedTime === null && rightEyeClosedTime === null) {
                leftEyeClosedTime = currentTime;
                rightEyeClosedTime = currentTime;
            }
            totalEyeClosureTime += (currentTime - leftEyeClosedTime) / 1000;
        } else {
            leftEyeClosedTime = null;
            rightEyeClosedTime = null;
            totalEyeClosureTime = 0;
        }

        if (totalEyeClosureTime > 3 && !alertShown) {
            displayWarning("Eyes closed for too long. Please stay alert.");
            alertShown = true;
        } else {
            alertShown = false;
        }
    }

    // Display eye closure time on the canvas
    function displayEyeClosureTime() {
        ctx.fillStyle = 'white';
        ctx.font = '16px Arial';
        ctx.fillText(`Total Eye Closure Time: ${totalEyeClosureTime.toFixed(2)}s`, 10, 30);
    }

    // Display warnings on the canvas
    function displayWarning(message) {
        ctx.fillStyle = 'rgba(255, 0, 0, 0.8)';
        ctx.font = '20px Arial';
        ctx.fillText(message, 20, VIDEO_HEIGHT / 2);
    }

    // Process eyes for iris detection
    function processEyes(landmarks) {
        const leftEyeLandmarks = [468, 469, 470, 471, 472];
        const rightEyeLandmarks = [473, 474, 475, 476, 477];
        drawIris(landmarks, leftEyeLandmarks, 'blue');
        drawIris(landmarks, rightEyeLandmarks, 'green');
    }

    // Draw iris
    function drawIris(landmarks, indices, color) {
        ctx.fillStyle = color;
        indices.forEach((index) => {
            const x = landmarks[index].x * VIDEO_WIDTH;
            const y = landmarks[index].y * VIDEO_HEIGHT;
            ctx.beginPath();
            ctx.arc(x, y, 3, 0, 2 * Math.PI);
            ctx.fill();
        });
    }

    // Continuously process video frames
    async function detectFaces() {
        await faceMesh.send({ image: video });
        requestAnimationFrame(detectFaces);
    }

    // Main function
    async function main() {
        await setupCamera();
        canvas = document.getElementById('output');
        ctx = canvas.getContext('2d');
        canvas.width = VIDEO_WIDTH;
        canvas.height = VIDEO_HEIGHT;
        initializeFaceMesh();
        detectFaces();
    }

    main();
</script>
</body>
</html>
